{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment : Feature Engineering"
      ],
      "metadata": {
        "id": "xlVpbHR6Favn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "\n",
        "parameter:\n",
        "* It is fixed (does not change for a given population).\n",
        "* It is usually unknown, because measuring the whole population is often  impractical.\n",
        "* Parameters are typically denoted by Greek letters.\n",
        "\n"
      ],
      "metadata": {
        "id": "1KnzvfPpFehO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        "What does negative correlation mean?\n",
        "\n",
        "* It shows whether variables increase or decrease together or move in opposite directions.\n",
        "* The most common measure is the correlation coefficient (r).\n",
        "* The value of r ranges from –1 to +1.\n",
        "\n",
        "negative correlation mean:\n",
        "* A negative correlation means that as one variable increases, the other decreases, and vice versa.\n",
        "\n",
        "Example of negative correlation:\n",
        "* As price of a product increases, demand decreases\n",
        "* As number of absences increases, exam scores decrease"
      ],
      "metadata": {
        "id": "vjsXp1LsGoH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "* Machine Learning (ML) is a branch of Artificial Intelligence (AI) that enables computer systems to learn patterns from data and improve their performance on a task without being explicitly programmed.\n",
        "\n",
        "Main Components of ML:\n",
        "* Data\n",
        "* Features\n",
        "* Model\n",
        "* Algorithm\n",
        "* Loss Function (Cost Function)\n",
        "* Training Process\n",
        "* Evaluation Metrics\n",
        "* Prediction / Inference\n",
        "\n",
        "Data → Features → Algorithm → Model → Evaluation → Prediction"
      ],
      "metadata": {
        "id": "fLHf5jHLHO0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "* The loss value tells us how wrong a model’s predictions are, so it is a key indicator of whether a model is learning well or not.\n",
        "\n",
        "How loss value helps judge a model:\n",
        "* Measures prediction error\n",
        "* Shows learning progress during training\n",
        "* Helps detect underfitting and overfitting\n",
        "* Guides model optimization\n",
        "* Enables fair model comparison"
      ],
      "metadata": {
        "id": "lKLPi9nSIBJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "\n",
        "Continuous Variables:\n",
        "* Continuous variables are variables that can take any numerical value within a given range.\n",
        "* Measured, not counted\n",
        "* Can have decimal values\n",
        "* Represent quantities\n",
        "\n",
        "Examples:\n",
        "* Height (165.5 cm)\n",
        "* Weight (62.3 kg)\n",
        "* Temperature (36.7°C)\n",
        "\n",
        "Categorical Variables:\n",
        "* Categorical variables represent qualities or categories rather than numerical quantities.\n",
        "* Values are labels or groups\n",
        "* Arithmetic operations are not meaningful\n",
        "* Can be finite or limited categories\n",
        "\n",
        "Types of Categorical Variables:\n",
        "* Nominal Variables\n",
        "* Ordinal Variables"
      ],
      "metadata": {
        "id": "iQGGWdkhImjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "* In Machine Learning, categorical variables must be converted into numerical form, because most algorithms work only with numbers. This process is called categorical encoding.\n",
        "\n",
        "Common Techniques to Handle Categorical Variables:\n",
        "* Label Encoding\n",
        "* One-Hot Encoding\n",
        "* Ordinal Encoding\n",
        "* Target Encoding (Mean Encoding)\n",
        "* Frequency (Count) Encoding\n",
        "* Binary Encoding\n",
        "* Embedding (Deep Learning)"
      ],
      "metadata": {
        "id": "Wn86meEKJ1t4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "\n",
        "Training Dataset:\n",
        "* The training dataset is the portion of data used to train the model.\n",
        "* The model learns patterns, relationships, and parameters from this data.\n",
        "* The algorithm minimizes loss using the training data.\n",
        "\n",
        "Example:\n",
        "* If you are building a model to predict house prices, the model uses known features (size, location) and known prices from the training data to learn.\n",
        "\n",
        "Testing Dataset:\n",
        "* The testing dataset is used to evaluate the model’s performance after training.\n",
        "* To check how well the model generalizes to new, unseen data.\n",
        "* No learning happens on the test data.\n",
        "\n",
        "Example:\n",
        "* The trained model predicts prices for the test data, and predictions are compared with actual values to measure accuracy or error."
      ],
      "metadata": {
        "id": "akC46M2mKnNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "* sklearn.preprocessing is a module in the scikit-learn library that provides tools to prepare and transform data before feeding it into a machine learning model.\n",
        "\n",
        "Important:\n",
        "* Ensures all features are on a comparable scale\n",
        "* Converts categorical data into numerical form\n",
        "* Improves model accuracy and convergence speed\n",
        "* Prevents some features from dominating others"
      ],
      "metadata": {
        "id": "De06h8U4NoyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "* A test set is a portion of the dataset that is kept separate from training and used only to evaluate the final performance of a machine learning model.\n",
        "\n",
        "Purpose of a Test Set:\n",
        "* Evaluate model performance\n",
        "* Detect overfitting\n",
        "* Simulate real-world data\n"
      ],
      "metadata": {
        "id": "mKofofjPOIDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "* In Python, we commonly use train_test_split from sklearn.model_selection.\n",
        "\n",
        "How do you approach a Machine Learning problem?\n",
        "* Understand the Problem\n",
        "* Collect & Understand Data\n",
        "* Data Preprocessing\n",
        "* Feature Engineering\n",
        "* Choose a Model\n",
        "* Train the Model\n",
        "* Evaluate the Model\n",
        "* Improve the Model\n",
        "* Deploy & Monitor"
      ],
      "metadata": {
        "id": "ZcgoPFZOOpd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "* We perform Exploratory Data Analysis (EDA) before fitting a model to understand the data and avoid costly modeling mistakes.\n",
        "\n",
        "Why EDA is necessary before model fitting:\n",
        "* To understand the data structure\n",
        "* To detect missing values\n",
        "* To identify outliers and anomalies\n",
        "* To understand feature distributions\n",
        "* To study relationships between variables\n",
        "* To detect data leakag\n",
        "* To choose the right preprocessing steps\n",
        "* To avoid poor model performance\n"
      ],
      "metadata": {
        "id": "mv0d-9c-QO7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "* Correlation measures the degree and direction of association between two variables.\n",
        "* It's Ranges from –1 to +1\n",
        "* r = +1 → perfect positive correlation\n",
        "* r = –1 → perfect negative correlation\n",
        "* r = 0 → no linear relationship"
      ],
      "metadata": {
        "id": "6f8s8gOIQ8Jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "* Negative correlation indicates an inverse relationship where one variable increases as the other decreases.\n",
        "* The correlation coefficient r is less than 0 (r < 0).\n",
        "* A stronger negative value (closer to –1) means a stronger inverse relationship.\n"
      ],
      "metadata": {
        "id": "vYHoj5LxRnEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (14) How can you find correlation between variables in Python?\n",
        "# You can find correlation between variables in Python using libraries like pandas, NumPy, and seaborn. The most common and simplest method is using pandas.\n",
        "# Using pandas:\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Age': [22, 25, 30, 35],\n",
        "    'Salary': [30000, 35000, 40000, 45000]\n",
        "})\n",
        "correlation_matrix = data.corr()\n",
        "print(correlation_matrix)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0Op5QO6SLRJ",
        "outputId": "e7f7cdb4-8273-4451-f5ca-6fb52e8315b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Age    Salary\n",
            "Age     1.000000  0.993859\n",
            "Salary  0.993859  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "causation:\n",
        "* Causation (or causal relationship) occurs when a change in one variable directly causes a change in another variable.\n",
        "\n",
        "Difference between Correlation and Causation:\n",
        "\n",
        "Correlation:\n",
        "* Measures the strength and direction of a relationship between two variables\n",
        "* Variables move together, but one doesn’t necessarily cause the other\n",
        "* Correlation coefficient (r) quantifies it\n",
        "\n",
        "Example=> Ice cream sales ↑ and drowning incidents ↑ (they occur together)\n",
        "\n",
        "Causation:\n",
        "* Indicates that one variable directly affects another\n",
        "* One variable causes a change in the other\n",
        "* One variable causes a change in the other\n",
        "\n",
        "Example=> Smoking ↑ → Lung cancer ↑ (smoking causes lung cancer)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "So-2VewnUfWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "* An optimizer is an algorithm that adjusts the parameters (weights and biases) of a model to minimize the loss function during training.\n",
        "\n",
        "Types of Optimizers:\n",
        "\n",
        "Gradient Descent (GD):\n",
        "* Updates parameters using the gradient of the loss function\n",
        "Types of gradient descent:\n",
        "* Batch Gradient Descent\n",
        "* Stochastic Gradient Descent (SGD)\n",
        "* Mini-batch Gradient Descent\n",
        "\n",
        "Momentum:\n",
        "* Accelerates SGD by adding a fraction of previous update to current update\n",
        "* Helps overcome local minima and smooth updates\n",
        "\n",
        "AdaGrad (Adaptive Gradient):\n",
        "* Adjusts learning rate for each parameter individually\n",
        "* Large updates for infrequent parameters, small updates for frequent parameters\n",
        "Good for sparse data\n",
        "\n",
        "Example: Natural language processing (sparse word embeddings)\n",
        "\n",
        "RMSProp:\n",
        "* Improves AdaGrad by using moving average of squared gradients\n",
        "* Prevents learning rate from shrinking too much\n",
        "\n",
        "Example: Used in RNNs (LSTM/GRU) for time-series prediction\n",
        "\n",
        "Adam (Adaptive Moment Estimation):\n",
        "* Combines Momentum + RMSProp\n",
        "* Keeps moving average of gradients and squared gradients\n",
        "* Most widely used optimizer in deep learning"
      ],
      "metadata": {
        "id": "tYADFL24Vh4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?\n",
        "* sklearn.linear_model is a module in scikit-learn that provides linear models for regression and classification tasks.\n",
        "* Easy to use and interpret\n",
        "* Handles both regression and classification\n",
        "* Can include regularization to prevent overfitting (Ridge, Lasso, ElasticNet)\n"
      ],
      "metadata": {
        "id": "sDlbcXM6XRMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "* model.fit() is the method used to train (fit) a machine learning model on data.\n",
        "* It allows the model to learn patterns and relationships between input\n",
        "features and the target variable.\n",
        "\n",
        "When you call model.fit():\n",
        "* The model receives training data\n",
        "* It learns model parameters (e.g., weights, coefficients, splits)\n",
        "* It minimizes a loss/error function\n",
        "* The trained model becomes ready for prediction using model.predict()\n",
        "\n",
        "Required arguments\n",
        "\n",
        "X_train (Features / Input data)\n",
        "* A 2D array-like structure\n",
        "* Shape: (number_of_samples, number_of_features)\n",
        "* Can be a NumPy array, Pandas DataFrame, or sparse matrix\n",
        "\n",
        "example:\n",
        "X_train = [[22, 25000],\n",
        "           [35, 45000],\n",
        "           [40, 50000]]\n",
        "\n",
        "y_train (Target / Output labels)\n",
        "* A 1D array-like structure\n",
        "* Shape: (number_of_samples,)\n",
        "* Represents the value to be predicted\n",
        "\n",
        "example:\n",
        "y_train = [0, 1, 1]\n",
        "\n"
      ],
      "metadata": {
        "id": "NF795FqSYMPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "* model.predict() is used to make predictions using a trained model.\n",
        "After a model has been trained with model.fit(), predict() applies the learned patterns to new or unseen data.\n",
        "\n",
        "When you call model.predict():\n",
        "* The model uses the learned parameters (weights, coefficients, rules, etc.)\n",
        "* It processes the input features\n",
        "* It outputs predicted values or class labels\n",
        "\n",
        "X_test (Input features)\n",
        "* Data for which predictions are required\n",
        "* Shape: (number_of_samples, number_of_features)\n",
        "* Must have the same number of features as training data\n",
        "* Can be a NumPy array, Pandas DataFrame, or similar\n",
        "\n",
        "example:\n",
        "X_test = [[30, 42000],\n",
        "          [45, 55000]]\n",
        "\n"
      ],
      "metadata": {
        "id": "hNPtj8jXZHmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "\n",
        "Continuous Variables:\n",
        "* Continuous variables are numerical variables that can take any value within a range.\n",
        "* Measured, not counted\n",
        "* Can take decimal or fractional values\n",
        "* Represent quantities\n",
        "\n",
        "Examples:\n",
        "* Height (170.5 cm)\n",
        "* Weight (62.8 kg)\n",
        "* Temperature (36.6°C)\n",
        "\n",
        "Categorical Variables:\n",
        "* Categorical variables represent categories or labels, not numerical quantities.\n",
        "* Values are groups or classes\n",
        "* Arithmetic operations are not meaningful\n",
        "* Usually limited and finite\n",
        "\n",
        "Examples:\n",
        "* Gender → Male, Female\n",
        "* Blood group → A, B, AB, O\n",
        "* City → Delhi, Mumbai, Bangalore"
      ],
      "metadata": {
        "id": "KZDZguWl4eJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "* Feature scaling ensures that all features contribute fairly to the model.\n",
        "\n",
        "Why is feature scaling needed?\n",
        "* Consider this example:\n",
        "\n",
        "Feature\tRange\n",
        "\n",
        "Age\t= 18 – 60\n",
        "\n",
        "Salary\t= 20,000 – 1,000,000\n",
        "\n",
        "Without scaling, Salary will dominate the learning process because its values are much larger.\n",
        "\n",
        "How feature scaling helps in Machine Learning:\n",
        "* Faster convergence\n",
        "* Prevents feature dominance\n",
        "* Improves model accuracy\n",
        "* Correct distance calculation\n",
        "* Improves numerical stability\n",
        "\n"
      ],
      "metadata": {
        "id": "qvlxgN2pDs7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  How do we perform scaling in Python?\n",
        " * Common Ways to Perform Scaling in Python\n",
        "\n",
        "Standardization (StandardScaler)\n",
        "* Scales data to have:\n",
        "* Mean = 0\n",
        "* Standard deviation = 1\n",
        "* Best for: Linear Regression, Logistic Regression, SVM, KNN, PCA\n",
        "\n",
        "Min-Max Scaling (MinMaxScaler)\n",
        "* Scales data to a fixed range (usually 0 to 1)\n",
        "* Best for: Neural Networks, distance-based models\n",
        "\n",
        "Robust Scaling (RobustScaler)\n",
        "* Uses median and IQR, making it robust to outliers\n",
        "* Best for: Data with outliers\n",
        "\n",
        "MaxAbs Scaling\n",
        "* Scales by maximum absolute value (range –1 to 1)"
      ],
      "metadata": {
        "id": "wNXiBQ__7WDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "* sklearn.preprocessing is a module in the scikit-learn library that provides tools to prepare and transform data before feeding it into a machine learning model.\n",
        "\n",
        "importance:\n",
        "* Ensures features are on a similar scale\n",
        "* Converts categorical data into numerical form\n",
        "* Improves model accuracy and convergence\n",
        "\n"
      ],
      "metadata": {
        "id": "gFdT3tAF729T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(24) How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "# In Python, we usually split data into training and testing sets so that a model is trained on one part of the data and evaluated on unseen data. This helps us check how well the model generalizes.\n",
        "# Training set → Used to fit (train) the model\n",
        "# Testing set → Used to evaluate model performance\n",
        "# Prevents overfitting\n",
        "# Gives an unbiased estimate of model accuracy\n",
        "\n",
        "# Common method: train_test_split (scikit-learn)\n",
        "# The most widely used approach is from scikit-learn.\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Load your dataset\n",
        "data = pd.read_csv(\"/content/dataset.csv\")\n",
        "\n",
        "#Separate features and target\n",
        "X = data.drop(\"target\", axis=1)  # input features\n",
        "y = data[\"target\"]               # output/label\n",
        "\n",
        "#Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,      # 20% data for testing\n",
        "    random_state=42,    # ensures reproducibility\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# X_train, y_train → Used to train the model\n",
        "# X_test, y_test → Used to evaluate the model on unseen data"
      ],
      "metadata": {
        "id": "ByIABwSZ5TiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "* Data encoding is the process of converting categorical (non-numeric) data into numerical form so that machine learning algorithms can understand and process it.\n",
        "\n",
        "Importance:\n",
        "* ML algorithms perform mathematical calculations\n",
        "* Text labels like Male, Female, Yes, No cannot be directly used\n",
        "* Encoding helps models learn patterns correctly\n",
        "\n",
        "Types of Data Encoding Techniques:\n",
        "1. Label Encoding:\n",
        "Assigns a unique integer to each category\n",
        "| Gender | Encoded |\n",
        "| ------ | ------- |\n",
        "| Male   | 1       |\n",
        "| Female | 0       |\n",
        "\n",
        "2. One-Hot Encoding\n",
        "* Creates binary columns (0/1) for each category.\n",
        "\n",
        "* Delhi      → 1 0 0\n",
        "* Mumbai    → 0 1 0\n",
        "* Bangalore → 0 0 1\n",
        "\n",
        "3. Ordinal Encoding\n",
        "* Used when categories have a natural ranking.\n",
        "* Low → 1\n",
        "* Medium → 2\n",
        "* High → 3\n",
        "\n",
        "4. Binary Encoding\n",
        "* Converts categories to binary numbers\n",
        "* Uses fewer columns than one-hot encoding\n",
        "\n",
        "5. Target (Mean) Encoding\n",
        "* Replaces categories with the average value of the target variable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "faIbEcqt_VOG"
      }
    }
  ]
}